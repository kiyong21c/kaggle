{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyong21c/kaggle/blob/main/20220705_pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g8C4TYr9sKTr",
        "outputId": "dcc28651-e122-4d93-e28a-b9837b0b2248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### numpy와 torch로 구현해보는 backward()/no_grad()"
      ],
      "metadata": {
        "id": "zJqKVUX0a5vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch X, Numpy O\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# f = w * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0 # 시작은 0으로\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x, y, y_predicted):\n",
        "    return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = fowrard pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradient\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # update weights\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    if epoch % 1 == 0: # 모든 epoch에 대해\n",
        "        print(f'epoch {epoch+1}: w ={w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after traning: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "PTdJEgIroW-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy X pytorch O\n",
        "\n",
        "import torch\n",
        "\n",
        "# f = w * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) # 시작은 0으로\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# # gradient\n",
        "# # MSE = 1/N * (w*x - y)**2\n",
        "# # dJ/dw = 1/N 2x (w*x - y)\n",
        "# def gradient(x, y, y_predicted):\n",
        "#     return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = fowrard pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    l.backward() # dl/dw\n",
        "    \n",
        "    # update weights\n",
        "    with torch.no_grad():\n",
        "        # with 구문 안에서 새로 생성된 텐서(파라미터 w)들은 requires_grad=False 상태가 되어, 메모리 사용량을 아껴준다.\n",
        "        # 파라미터 w의 gradient를 계산하지 않겠다\n",
        "        w -= learning_rate * w.grad # lr * dl/dw\n",
        "        # 위에서 새로 정의된 w의 grad가 아닌, 우항의 w.grad(전역변수 w의 grad)만 유효하다\n",
        "\n",
        "    # zero gradients\n",
        "    w.grad.zero_()  # w 갱신 후 dl/dw를 초기화(0) 해주자\n",
        "\n",
        "    if epoch % 1 == 0: # 모든 epoch에 대해\n",
        "        print(f'epoch {epoch+1}: w ={w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after traning: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "4BNm5qhyUpDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한땀씩\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "# This is the parameter we want to optimize -> requires_grad=True\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass to compute loss_1\n",
        "y_predicted = w * x\n",
        "loss = (y_predicted - y)**2\n",
        "print('loss :', loss)\n",
        "\n",
        "# backward pass to compute gradient dLoss/dw\n",
        "loss.backward()\n",
        "print(w.grad)   # -2\n",
        "\n",
        "# learning_rate\n",
        "lr = 0.01\n",
        "\n",
        "# update parameter\n",
        "with torch.no_grad():\n",
        "    w -= lr * w.grad    # 1 → 1 - (-0.01)*(-2)\n",
        "\n",
        "print(w)   # 1.02\n",
        "print(f'before w.grad.zero_() :', w.grad) # -2\n",
        "w.grad.zero_()\n",
        "print(f'after w.grad.zero_() :', w.grad)  # 0\n",
        "print('w :',w) # 1.02 : w.grad가 0가 된것, w는 갱신된 상태\n",
        "\n",
        "# forward pass to compute loss_2\n",
        "y_predicted = w * x\n",
        "loss = (y_predicted - y)**2\n",
        "print('loss :', loss)\n",
        "\n",
        "# backward pass to compute gradient dLoss/dw\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# 1.순전파 : loss 계산\n",
        "\n",
        "# 2.역전파 : loss.backward() 하면서 w.grad(=dLoss/dw) 계산됨\n",
        "\n",
        "# 3.파라미터 업데이트(w.grad계산 꺼줌) : with torch.no_grad(): \n",
        "\n",
        "# 4.w.grad(dLoss/dw)를 0으로 초기화 : w.grad.zero_() , 갱신된 w를 통해서 새로운 y_predicted 및 loss가 계산되므로 역전파시 w.grad도 다시 계산되어야 하므로(0안하면 누적됨)"
      ],
      "metadata": {
        "id": "7ze5A6SogH5v",
        "outputId": "38091eaa-fed2-4e1b-e29d-66f299603052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n",
            "tensor(1.0200, requires_grad=True)\n",
            "before w.grad.zero_() : tensor(-2.)\n",
            "after w.grad.zero_() : tensor(0.)\n",
            "w : tensor(1.0200, requires_grad=True)\n",
            "loss : tensor(0.9604, grad_fn=<PowBackward0>)\n",
            "tensor(-1.9600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer 활용\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x \n",
        "\n",
        "# here : f = 2 * x\n",
        "\n",
        "# 0) Training samples\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) # torch.Size([4])\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32) # torch.Size([4])\n",
        "\n",
        "# 1) Design Model: Weights to optimize and forward function\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}') # 0 : w 초기값이 0이므로\n",
        "\n",
        "# 2) Define loss and optimizer\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "# callable function\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)  # optimizer가 w, lr과 관련있어 w를 갱신하고, grad를 초기화할 수 있음\n",
        "optimizer   # optimizer 출력 : 설정된 세팅이 나옴\n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_predicted = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    with torch.no_grad():   # w 갱신시에는 w.grad계산 꺼주듯이 optimizer 갱신시에도? with torch.no_grad(): 구문 안해도 됨\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "        # optimizer.grad : 출력 안됨\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 0:     # 10회 단위로 출력\n",
        "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
        "\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "id": "ZjT6MlcDgI5-",
        "outputId": "6a282a1b-b9e7-4b5f-846f-f4a0ac3c5a2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4])\n",
            "Prediction before training: f(5) = 0.000\n",
            "epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=<MseLossBackward0>)\n",
            "epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=<MseLossBackward0>)\n",
            "epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
            "epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=<MseLossBackward0>)\n",
            "epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=<MseLossBackward0>)\n",
            "epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=<MseLossBackward0>)\n",
            "epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=<MseLossBackward0>)\n",
            "epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=<MseLossBackward0>)\n",
            "epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=<MseLossBackward0>)\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model 활용\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x \n",
        "\n",
        "# here : f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "# 모델에 사용될 데이터는 2차원 이어야함\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) \n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) \n",
        "\n",
        "n_samples, n_features = X.shape # torch.Size([4, 1])\n",
        "print(f'#samples: {n_samples}, #features: {n_features}')\n",
        "\n",
        "# 0) create a test sample\n",
        "X_test = torch.tensor([5], dtype=torch.float32) # X_test.shape : torch.Size([1]) 넘파이랑 다른 형식\n",
        "\n",
        "# 1) Design Model, the model has to implement the forward pass!\n",
        "# Here we can use a built-in model from PyTorch\n",
        "input_size = n_features # 1\n",
        "output_size = n_features # 1\n",
        "\n",
        "# we can call this model with samples X\n",
        "model = nn.Linear(input_size, output_size) # 모델생성\n",
        "\n",
        "'''\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        # define diferent layers\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "model = LinearRegression(input_size, output_size)\n",
        "'''\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}') # 출력할때 마다 다른값\n",
        "# model(X_test) : tensor([4.1130], grad_fn=<AddBackward0>)\n",
        "# model(X_test).item() : 4.113\n",
        "\n",
        "\n",
        "# 2) Define loss and optimizer\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss() # 모델을 통해 나온값과 실제값의 차이를 MSE 방식으로 처리하겠다\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # 모델의 파라미터(w, b)를 갱신/관리를 SGD 방식으로 하겠다\n",
        "# list(model.parameters()) : [Parameter containing : tensor([[-0.0085]], requires_grad=True), Parameter containing : tensor([-0.9986], requires_grad=True)]\n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass with our model\n",
        "    y_predicted = model(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step() # w, b 갱신\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    optimizer.zero_grad() # dloss/dw, dloss/db 초기화(0)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        [w, b] = model.parameters() # 1)제너레이터를 변수에 나눠서 할당하거나, 2)list화 하거나\n",
        "        print(f'epoch : {epoch+1},  w = {w[0].item():.3f},  b = {b[0].item():.3f},  loss = {l.item():.3f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "id": "_RnlyRLBo792",
        "outputId": "cf8484d8-d116-43cd-9aae-b33af52ef073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#samples: 4, #features: 1\n",
            "Prediction before training: f(5) = -5.457\n",
            "epoch : 1,  w = -0.507,  b = -0.388,  loss = 75.157\n",
            "epoch : 11,  w = 1.482,  b = 0.275,  loss = 1.971\n",
            "epoch : 21,  w = 1.805,  b = 0.371,  loss = 0.076\n",
            "epoch : 31,  w = 1.861,  b = 0.377,  loss = 0.026\n",
            "epoch : 41,  w = 1.873,  b = 0.369,  loss = 0.023\n",
            "epoch : 51,  w = 1.878,  b = 0.358,  loss = 0.022\n",
            "epoch : 61,  w = 1.882,  b = 0.348,  loss = 0.020\n",
            "epoch : 71,  w = 1.885,  b = 0.338,  loss = 0.019\n",
            "epoch : 81,  w = 1.889,  b = 0.328,  loss = 0.018\n",
            "epoch : 91,  w = 1.892,  b = 0.318,  loss = 0.017\n",
            "Prediction after training: f(5) = 9.783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Prepare data : noise가 들어간 선형 데이터셋 생성\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
        "\n",
        "# Prepare data 시각화\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(X_numpy, y_numpy)\n",
        "\n",
        "# cast to float Tensor\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32)) # torch.Size([100])\n",
        "y = y.view(y.shape[0], 1) # y.view(100, 1) : 1차원의 tensor [100]를 2차원 tensor로 [100, 1]\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
        "\n",
        "# 파라미터 확인\n",
        "w, b = model.parameters()\n",
        "print('최초 w =',w[0].item(), '최초 b =',b[0].item())\n",
        "# 최초 파라미터 갱신전 plot\n",
        "plt.plot(X, w[0].item()*X + b[0].item())\n",
        "\n",
        "# 3) Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "    \n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step() # optimizer 내부로직에 의해 w, b가 갱신됨\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "print('최종 w =',w[0].item(), '최종 b =',b[0].item())\n",
        "# 최종 파라미터 갱신후 plot(w, b를 이용해 그리는 방법)\n",
        "plt.plot(X, w[0].item()*X + b[0].item())\n",
        "\n",
        "# Plot\n",
        "predicted = model(X).detach().numpy() # detach() : gradient가 전파되지 않는 텐서를 복사(= with torch.no_grad(): 와 같은 개념)\n",
        "\n",
        "# 최종 파라미터 갱신후 plot(model(X))를 이용해 그리는 방법)\n",
        "plt.plot(X_numpy, predicted, 'b') # numpy아닌 torch로도 그려짐\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HaG4HOkz7qKZ",
        "outputId": "d20d77cc-1f20-4d40-a336-ec51b81b61b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w = -0.9096072912216187 최초 b = 0.6855100393295288\n",
            "epoch: 10, loss = 4149.9565\n",
            "epoch: 20, loss = 2922.9468\n",
            "epoch: 30, loss = 2086.3574\n",
            "epoch: 40, loss = 1515.8420\n",
            "epoch: 50, loss = 1126.6963\n",
            "epoch: 60, loss = 861.2088\n",
            "epoch: 70, loss = 680.0491\n",
            "epoch: 80, loss = 556.4081\n",
            "epoch: 90, loss = 472.0081\n",
            "epoch: 100, loss = 414.3837\n",
            "최종 w = 63.70635986328125 최종 b = 0.48071110248565674\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# gradient computation etc. not efficient for whole data set\n",
        "# -> divide dataset into small batches\n",
        "\n",
        "'''\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # loop over all batches\n",
        "    for i in range(total_batches):\n",
        "        batch_x, batch_y = ...\n",
        "'''\n",
        "\n",
        "# epoch = one forward and backward pass of ALL training samples\n",
        "# batch_size = number of training samples used in one forward/backward pass\n",
        "# number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of sampes\n",
        "# e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\n",
        "\n",
        "# --> DataLoader can do the batch computation for us\n",
        "\n",
        "# Implement a custom Dataset:\n",
        "# inherit Dataset\n",
        "# implement __init__ , __getitem__ , and __len__\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "    # Dataset 클래스를 상속받으면서, 1)__getitem__, 2)__len__ 재정의(오버라이딩)\n",
        "    def __init__(self):\n",
        "        # Initialize data, download, etc.\n",
        "        # read with numpy or pandas\n",
        "        xy = np.loadtxt('/content/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "        # here the first column is the class label, the rest are the features\n",
        "        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        # print('__getitem__ 메서드 호출됨')\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        # print('__len__ 메서드 호출됨')\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "# create dataset\n",
        "dataset = WineDataset()\n",
        "\n",
        "# get first sample and unpack\n",
        "first_data = dataset[0] # class 매직메소드인 __getitem__이 호출됨\n",
        "features, labels = first_data\n",
        "print('-'*10, '첫번째 데이터셋', '-'*10)\n",
        "print('features :',features, 'labels :',labels)\n",
        "\n",
        "# Load whole dataset with DataLoader\n",
        "# shuffle: shuffle data, good for training\n",
        "# num_workers: faster loading with multiple subprocesses\n",
        "# !!! IF YOU GET AN ERROR DURING LOADING, SET num_workers TO 0 !!!\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=4,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "# convert to an iterator and look at one random sample\n",
        "dataiter = iter(train_loader) # train_loader를 출력하기위해\n",
        "data = dataiter.next()\n",
        "features, labels = data # train_loader를 features, labels로 반환할 수 있음\n",
        "print('-'*10, 'train_loader(shuffled) sample(random batch)', '-'*10)\n",
        "print(features, labels) # batch_size에 따라 4개의 데이터셋을 가짐\n",
        "\n",
        "# Dummy Training loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)    # class 매직메소드인 __len__이 호출됨\n",
        "n_iterations = math.ceil(total_samples/4)   # total_samples(178)/batch_size(4)를 올림\n",
        "print('전체 데이터 개수 :', total_samples, ', 전체 데이터를 학습하기 위한 batch의 반복횟수 :', n_iterations)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader): # i는 iteration이 되겠고, inputs는 피처, labels는 타겟\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        # here: 178 samples, batch_size = 4, n_iters=178/4=44.5 -> 45 iterations\n",
        "        # Run your training process\n",
        "        if (i+1) % 5 == 0:\n",
        "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')\n",
        "\n",
        "# # some famous datasets are available in torchvision.datasets\n",
        "# # e.g. MNIST, Fashion-MNIST, CIFAR10, COCO\n",
        "\n",
        "# train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "#                                            train=True, \n",
        "#                                            transform=torchvision.transforms.ToTensor(),  \n",
        "#                                            download=True)\n",
        "\n",
        "# train_loader = DataLoader(dataset=train_dataset, \n",
        "#                                            batch_size=3, \n",
        "#                                            shuffle=True)\n",
        "\n",
        "# # look at one random sample\n",
        "# dataiter = iter(train_loader)\n",
        "# data = dataiter.next()\n",
        "# inputs, targets = data\n",
        "# print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "id": "s-9Buzk2FBiX",
        "outputId": "f6a5a95c-c5a4-47aa-f4ce-ff30bc6a8a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- 첫번째 데이터셋 ----------\n",
            "features : tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) labels : tensor([1.])\n",
            "---------- train_loader(shuffled) sample(random batch) ----------\n",
            "tensor([[1.2640e+01, 1.3600e+00, 2.0200e+00, 1.6800e+01, 1.0000e+02, 2.0200e+00,\n",
            "         1.4100e+00, 5.3000e-01, 6.2000e-01, 5.7500e+00, 9.8000e-01, 1.5900e+00,\n",
            "         4.5000e+02],\n",
            "        [1.1760e+01, 2.6800e+00, 2.9200e+00, 2.0000e+01, 1.0300e+02, 1.7500e+00,\n",
            "         2.0300e+00, 6.0000e-01, 1.0500e+00, 3.8000e+00, 1.2300e+00, 2.5000e+00,\n",
            "         6.0700e+02],\n",
            "        [1.3510e+01, 1.8000e+00, 2.6500e+00, 1.9000e+01, 1.1000e+02, 2.3500e+00,\n",
            "         2.5300e+00, 2.9000e-01, 1.5400e+00, 4.2000e+00, 1.1000e+00, 2.8700e+00,\n",
            "         1.0950e+03],\n",
            "        [1.1620e+01, 1.9900e+00, 2.2800e+00, 1.8000e+01, 9.8000e+01, 3.0200e+00,\n",
            "         2.2600e+00, 1.7000e-01, 1.3500e+00, 3.2500e+00, 1.1600e+00, 2.9600e+00,\n",
            "         3.4500e+02]]) tensor([[2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "전체 데이터 개수 : 178 , 전체 데이터를 학습하기 위한 batch의 반복횟수 : 45\n",
            "Epoch: 1/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n",
            "Epoch: 2/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target   # X.shape : (569, 30), y.shape : (569,)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test) # X_train으로 훈련하여 X_test에도 적용\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1) # view() : 넘파이의 reshape()과 같다\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b , sigmoid at the end\n",
        "class Model(nn.Module):\n",
        "    # nn.Module을 상속받으면서, 1)forward 모듈 재정의(오버라이딩)\n",
        "    def __init__(self, n_input_features):\n",
        "        # super(Model, self).__init__()   # # nn.Module 상속\n",
        "        super().__init__()   # # nn.Module 상속\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):   # nn.Module 클래스에 있는 forward 모듈을 재정의(오버라이딩) 해야함\n",
        "    # 모델생성 후 self.forward() 하지 않아도 nn.Module의 영향으로 자동으로 호출되는 메서드\n",
        "        y_pred = torch.sigmoid(self.linear(x))  # torch가 제공하는 sigmoid()\n",
        "        return y_pred\n",
        "\n",
        "model = Model(n_features)\n",
        "# 파라미터 확인\n",
        "w, b = model.parameters()\n",
        "print('최초 w =',w[0], '최초 b =',b[0])\n",
        "\n",
        "# # 2) Loss and optimizer\n",
        "# num_epochs = 100\n",
        "# learning_rate = 0.01\n",
        "# criterion = nn.BCELoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # 3) Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Forward pass and loss\n",
        "#     y_pred = model(X_train)\n",
        "#     loss = criterion(y_pred, y_train)\n",
        "\n",
        "#     # Backward pass and update\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # zero grad before new step\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     if (epoch+1) % 10 == 0:\n",
        "#         print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     y_predicted = model(X_test)\n",
        "#     y_predicted_cls = y_predicted.round()\n",
        "#     acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "#     print(f'accuracy: {acc.item():.4f}')"
      ],
      "metadata": {
        "id": "wjYRTvOmaQvV",
        "outputId": "649eae0e-a93b-40a2-c4d0-c22f8d848ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w = tensor([ 0.0368, -0.1810,  0.0234, -0.0501,  0.0363,  0.1530, -0.1431, -0.0724,\n",
            "         0.0855, -0.1682, -0.1096, -0.0773,  0.1293, -0.1557,  0.0532,  0.0395,\n",
            "        -0.1672,  0.0590,  0.0078, -0.1113,  0.0908, -0.0412, -0.0671, -0.1534,\n",
            "         0.1595,  0.0268,  0.0896,  0.0304, -0.1641, -0.0221],\n",
            "       grad_fn=<SelectBackward0>) 최초 b = tensor(-0.1606, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vNvLYXEZEZln"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}